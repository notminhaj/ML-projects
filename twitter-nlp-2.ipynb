{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2510329,"sourceType":"datasetVersion","datasetId":1520310}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-05T01:16:54.032836Z","iopub.execute_input":"2024-09-05T01:16:54.033103Z","iopub.status.idle":"2024-09-05T01:16:54.397926Z","shell.execute_reply.started":"2024-09-05T01:16:54.033072Z","shell.execute_reply":"2024-09-05T01:16:54.396971Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/twitter-entity-sentiment-analysis/twitter_validation.csv\n/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv\")\ndata[\"Positive\"].unique","metadata":{"execution":{"iopub.status.busy":"2024-09-05T01:16:54.399504Z","iopub.execute_input":"2024-09-05T01:16:54.399886Z","iopub.status.idle":"2024-09-05T01:16:54.726177Z","shell.execute_reply.started":"2024-09-05T01:16:54.399853Z","shell.execute_reply":"2024-09-05T01:16:54.725169Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<bound method Series.unique of 0        Positive\n1        Positive\n2        Positive\n3        Positive\n4        Positive\n           ...   \n74676    Positive\n74677    Positive\n74678    Positive\n74679    Positive\n74680    Positive\nName: Positive, Length: 74681, dtype: object>"},"metadata":{}}]},{"cell_type":"code","source":"# Rename the columns\ndata.columns = ['id', 'topic', 'sentiment', 'tweet']","metadata":{"execution":{"iopub.status.busy":"2024-09-05T01:16:54.727351Z","iopub.execute_input":"2024-09-05T01:16:54.727672Z","iopub.status.idle":"2024-09-05T01:16:54.732262Z","shell.execute_reply.started":"2024-09-05T01:16:54.727638Z","shell.execute_reply":"2024-09-05T01:16:54.731394Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset, DatasetDict\n\n# Convert \"Irrelevant\" class to \"Neutral\"\ndata['sentiment'] = data['sentiment'].replace('Irrelevant', 'Neutral')\n\n# Encode sentiment labels to integers\nlabel_mapping = {'positive': 0, 'negative': 1, 'neutral': 2}\ndata['label'] = data['sentiment'].str.lower().map(label_mapping)\n\n# Drop any rows with missing labels or tweets\ndata = data.dropna(subset=['label', 'tweet'])\n\n# Split the data into training and validation sets\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['label'])\n\n# Convert pandas DataFrames to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_data[['tweet', 'label']])\nval_dataset = Dataset.from_pandas(val_data[['tweet', 'label']])\n\n# Create a DatasetDict for the Trainer\ndataset = DatasetDict({'train': train_dataset, 'validation': val_dataset})\n\n# Step 2: Fine-tune the CardiffNLP Twitter model\n\n# Load the tokenizer and model\nmodel_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=3,\n)\n\n# Tokenize the data\ndef preprocess_function(examples):\n    # Tokenizer expects a list of texts for batching\n    return tokenizer(examples['tweet'], truncation=True, padding='max_length', max_length=128)\n\n# Apply the tokenizer to the datasets\nencoded_train_dataset = train_dataset.map(preprocess_function, batched=True)\nencoded_val_dataset = val_dataset.map(preprocess_function, batched=True)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_strategy=\"epoch\",  # Save model checkpoints at each epoch,\n    report_to=\"none\"\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_train_dataset,\n    eval_dataset=encoded_val_dataset,\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T01:16:54.734613Z","iopub.execute_input":"2024-09-05T01:16:54.734919Z","iopub.status.idle":"2024-09-05T02:21:56.139456Z","shell.execute_reply.started":"2024-09-05T01:16:54.734885Z","shell.execute_reply":"2024-09-05T02:21:56.138446Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be7de0b7f3fe4b29bb7418670d3b6aa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"714ab0a425e144f6bfb40cdc91cdd13a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48202713055547debe68acee58811320"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36b86267e3f141bb943365a85d64a4a1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/982 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb61b56186514463b7a7877b848299c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec9815002ce74ea594e142ba590e8873"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/59196 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e8a93e189964948be7031f7b542cbae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14799 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b78ac06d817f48a194a1943ad06b906d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5550' max='5550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5550/5550 1:03:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.596400</td>\n      <td>0.477630</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.310100</td>\n      <td>0.323425</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.192300</td>\n      <td>0.313151</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5550, training_loss=0.40124616090241855, metrics={'train_runtime': 3833.4021, 'train_samples_per_second': 46.326, 'train_steps_per_second': 1.448, 'total_flos': 1.1681446406870016e+16, 'train_loss': 0.40124616090241855, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the fine-tuned model and tokenizer\nmodel_save_path = \"/kaggle/working/fine_tuned_model\"\ntrainer.save_model(model_save_path)  # Saves the model\ntokenizer.save_pretrained(model_save_path)  # Saves the tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-09-05T02:21:56.140853Z","iopub.execute_input":"2024-09-05T02:21:56.141296Z","iopub.status.idle":"2024-09-05T02:21:58.858084Z","shell.execute_reply.started":"2024-09-05T02:21:56.141225Z","shell.execute_reply":"2024-09-05T02:21:58.857180Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/fine_tuned_model/tokenizer_config.json',\n '/kaggle/working/fine_tuned_model/special_tokens_map.json',\n '/kaggle/working/fine_tuned_model/sentencepiece.bpe.model',\n '/kaggle/working/fine_tuned_model/added_tokens.json',\n '/kaggle/working/fine_tuned_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Load the fine-tuned model and tokenizer\nmodel_save_path = \"/kaggle/working/fine_tuned_model\"  # Path where the model was saved\ntokenizer = AutoTokenizer.from_pretrained(model_save_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_save_path)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T02:21:58.859390Z","iopub.execute_input":"2024-09-05T02:21:58.860197Z","iopub.status.idle":"2024-09-05T02:21:59.778453Z","shell.execute_reply.started":"2024-09-05T02:21:58.860152Z","shell.execute_reply":"2024-09-05T02:21:59.777141Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Load the validation dataset\nvalidation_data_path = \"/kaggle/input/twitter-entity-sentiment-analysis/twitter_validation.csv\"\nvalidation_data = pd.read_csv(validation_data_path)\n\n# Add columns as specified\nvalidation_data.columns = ['id', 'topic', 'sentiment', 'tweet']\n\n# Convert \"Irrelevant\" class to \"Neutral\"\nvalidation_data['sentiment'] = validation_data['sentiment'].replace('Irrelevant', 'Neutral')\n\n# Encode sentiment labels to integers\nlabel_mapping = {'positive': 0, 'negative': 1, 'neutral': 2}\nvalidation_data['label'] = validation_data['sentiment'].str.lower().map(label_mapping)\n\n# Drop any rows with missing labels or tweets\nvalidation_data = validation_data.dropna(subset=['label', 'tweet'])\n\n# Move the model to the same device as the input tensors\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Tokenize the validation data\ndef preprocess_function(examples):\n    return tokenizer(examples['tweet'].tolist(), truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n\n# Tokenize the validation tweets\nencoded_validation_data = preprocess_function(validation_data)\n\n# Move the input tensors to the same device as the model\nencoded_validation_data = {key: value.to(device) for key, value in encoded_validation_data.items()}\n\n# Run predictions\nwith torch.no_grad():  # Disables gradient calculation to save memory\n    outputs = model(**encoded_validation_data)\n    predictions = outputs.logits.argmax(dim=-1).cpu().numpy()  # Move predictions back to CPU\n\n# Map the predicted labels back to their corresponding sentiment classes\nlabel_reverse_mapping = {0: 'Positive', 1: 'Negative', 2: 'Neutral'}\nvalidation_data['predicted_sentiment'] = [label_reverse_mapping[pred] for pred in predictions]\n\n# Display the results\nprint(\"Predictions on Validation Data:\")\nprint(validation_data[['tweet', 'sentiment', 'predicted_sentiment']].head(20))  # Display 20 example predictions","metadata":{"execution":{"iopub.status.busy":"2024-09-05T02:21:59.780083Z","iopub.execute_input":"2024-09-05T02:21:59.780407Z","iopub.status.idle":"2024-09-05T02:22:07.515959Z","shell.execute_reply.started":"2024-09-05T02:21:59.780372Z","shell.execute_reply":"2024-09-05T02:22:07.514971Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Predictions on Validation Data:\n                                                tweet sentiment  \\\n0   BBC News - Amazon boss Jeff Bezos rejects clai...   Neutral   \n1   @Microsoft Why do I pay for WORD when it funct...  Negative   \n2   CSGO matchmaking is so full of closet hacking,...  Negative   \n3   Now the President is slapping Americans in the...   Neutral   \n4   Hi @EAHelp I‚Äôve had Madeleine McCann in my cel...  Negative   \n5   Thank you @EAMaddenNFL!! \\n\\nNew TE Austin Hoo...  Positive   \n6   Rocket League, Sea of Thieves or Rainbow Six: ...  Positive   \n7   my ass still knee-deep in Assassins Creed Odys...  Positive   \n8   FIX IT JESUS ! Please FIX IT ! What In the wor...  Negative   \n9   The professional dota 2 scene is fucking explo...  Positive   \n10  Itching to assassinate \\n\\n#TCCGif #AssassinsC...  Positive   \n11  @FredTJoseph hey fred, Comcast cut the cable a...  Negative   \n12  CSGO WIngman (Im Silver dont bully) twitch.tv/...   Neutral   \n13  @NBA2K game sucks... down by 2 with 38 seconds...  Negative   \n14  Congrats to the NVIDIA NeMo team for the 1.0.0...  Positive   \n15                                  yeah and it‚Äôs fun  Positive   \n16                                     fuck my life üòÜ  Negative   \n17  happy birthday red dead redemption that shit c...  Positive   \n18  What does that say about Microsoft hardware & ...  Negative   \n19     The new @CallofDuty for ps5 is üî•üî•üî•üî•\\nOh God üò≠üòç  Negative   \n\n   predicted_sentiment  \n0              Neutral  \n1             Negative  \n2             Negative  \n3              Neutral  \n4             Negative  \n5             Positive  \n6             Positive  \n7             Positive  \n8             Negative  \n9             Positive  \n10            Positive  \n11            Negative  \n12             Neutral  \n13            Negative  \n14            Positive  \n15            Positive  \n16            Negative  \n17            Positive  \n18            Negative  \n19            Positive  \n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Calculate accuracy\ntrue_labels = validation_data['label'].values\naccuracy = accuracy_score(true_labels, predictions)\nprint(f\"Accuracy of the fine-tuned model on the validation dataset: {accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-05T02:22:07.517336Z","iopub.execute_input":"2024-09-05T02:22:07.517733Z","iopub.status.idle":"2024-09-05T02:22:07.525519Z","shell.execute_reply.started":"2024-09-05T02:22:07.517689Z","shell.execute_reply":"2024-09-05T02:22:07.524477Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Accuracy of the fine-tuned model on the validation dataset: 0.9630\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the original pre-trained model and tokenizer\noriginal_model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\"\noriginal_tokenizer = AutoTokenizer.from_pretrained(original_model_name)\noriginal_model = AutoModelForSequenceClassification.from_pretrained(original_model_name)\n\n# Move the original model to the same device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noriginal_model.to(device)\n\n# Tokenize the validation data using the original model's tokenizer\nencoded_validation_data = preprocess_function(validation_data)\n\n# Move the input tensors to the same device as the model\nencoded_validation_data = {key: value.to(device) for key, value in encoded_validation_data.items()}\n\n# Run predictions with the original model\nwith torch.no_grad():  # Disables gradient calculation to save memory\n    original_outputs = original_model(**encoded_validation_data)\n    original_predictions = original_outputs.logits.argmax(dim=-1).cpu().numpy()  # Move predictions back to CPU\n\n# Calculate accuracy for the original model\ntrue_labels = validation_data['label'].values\noriginal_accuracy = accuracy_score(true_labels, original_predictions)\nprint(f\"Accuracy of the original model on the validation dataset: {original_accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-05T02:22:07.526766Z","iopub.execute_input":"2024-09-05T02:22:07.527161Z","iopub.status.idle":"2024-09-05T02:22:20.642542Z","shell.execute_reply.started":"2024-09-05T02:22:07.527127Z","shell.execute_reply":"2024-09-05T02:22:20.641591Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the original model on the validation dataset: 0.2382\n","output_type":"stream"}]}]}